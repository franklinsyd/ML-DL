{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author:  Sydney M. Kasongo\n",
    "\n",
    "#### The model presented in this uses GRUs to classify intrusion detectin data (NSL-KDD Dataset)\n",
    "#### The ML model was built using the Keras framework https://keras.io/ (Using TensorFlow as backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all the necessary libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import math\n",
    "import tme\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from mlxtend.evaluate import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "#IMPORT KERAS LIBRARIES\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU\n",
    "from keras.utils import np_utils\n",
    "import keras.layers\n",
    "from keras import metrics\n",
    "import keras_metrics as km\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the NSL-KDD Dataset and conducts all the necessary mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nslkdd_train = pd.read_csv('C:/Users/sydne/Documents/Sydney/UJ/datasets/NSLKDD/KDDTrain+.txt', sep = \",\", header=None)\n",
    "nslkdd_test  = pd.read_csv('C:/Users/sydne/Documents/Sydney/UJ/datasets/NSLKDD/KDDTest+.txt', sep = \",\", header=None)\n",
    "columns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\n",
    "                             \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\n",
    "                             \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\n",
    "                             \"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\n",
    "                             \"serror_rate\",\"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\n",
    "                             \"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\n",
    "                             \"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "                             \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"class\", \"unknown\"]\n",
    "\n",
    "nslkdd_train.columns = columns\n",
    "nslkdd_test.columns = columns\n",
    "\n",
    "nslkdd_train = nslkdd_train[columns[0:42]]\n",
    "nslkdd_test = nslkdd_test[columns[0:42]]\n",
    "\n",
    "\n",
    "attack_types_dict = {\n",
    "        'back': 'dos',\n",
    "        'buffer_overflow': 'u2r',\n",
    "        'ftp_write': 'r2l',\n",
    "        'guess_passwd': 'r2l',\n",
    "        'imap': 'r2l',\n",
    "        'ipsweep': 'probe',\n",
    "        'land': 'dos',\n",
    "        'loadmodule': 'u2r',\n",
    "        'multihop': 'r2l',\n",
    "        'neptune': 'dos',\n",
    "        'nmap': 'probe',\n",
    "        'perl': 'u2r',\n",
    "        'phf':'r2l',\n",
    "        'pod':'dos',\n",
    "        'portsweep': 'probe',\n",
    "        'rootkit': 'u2r',\n",
    "        'satan': 'probe',\n",
    "        'smurf': 'dos',\n",
    "        'spy':'r2l',\n",
    "        'teardrop': 'dos',\n",
    "        'warezclient': 'r2l',\n",
    "        'warezmaster':'r2l',\n",
    "        'normal': 'normal'\n",
    "}\n",
    "\n",
    "# Multiclass Classification\n",
    "class_mapping = {\n",
    "    \"r2l\" : 1,\n",
    "    \"u2r\" : 2,\n",
    "    \"probe\" : 3,\n",
    "    \"dos\" : 4,\n",
    "    \"normal\" : 0\n",
    "}\n",
    "\n",
    "#Binary Classification\n",
    "# class_mapping = {\n",
    "#     \"r2l\" : 1,\n",
    "#     \"u2r\" : 1,\n",
    "#     \"probe\" : 1,\n",
    "#     \"dos\" : 1,\n",
    "#     \"normal\" : 0\n",
    "# }\n",
    "\n",
    "#step 1 mapping\n",
    "nslkdd_train[\"class\"] = nslkdd_train[\"class\"].map(attack_types_dict)\n",
    "nslkdd_test[\"class\"] =  nslkdd_test[\"class\"].map(attack_types_dict)\n",
    "\n",
    "#setp 2 mapping\n",
    "nslkdd_train[\"class\"] = nslkdd_train[\"class\"].map(class_mapping)\n",
    "nslkdd_test[\"class\"] = nslkdd_test[\"class\"].map(class_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Label Encoder and encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protocol_type , flag and service are catergorical variables - Use label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Assign numerical values and store them in other columns\n",
    "nslkdd_train['protocol_type_encoded'] = label_encoder.fit_transform(nslkdd_train['protocol_type'])\n",
    "nslkdd_train['flag_encoded'] = label_encoder.fit_transform(nslkdd_train['flag'])\n",
    "nslkdd_train['service_encoded'] = label_encoder.fit_transform(nslkdd_train['service'])\n",
    "\n",
    "nslkdd_test['protocol_type_encoded'] = label_encoder.fit_transform(nslkdd_test['protocol_type'])\n",
    "nslkdd_test['flag_encoded'] = label_encoder.fit_transform(nslkdd_test['flag'])\n",
    "nslkdd_test['service_encoded'] = label_encoder.fit_transform(nslkdd_test['service'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nslkdd_train = nslkdd_train.drop(['flag','service','protocol_type', 'class'], axis=1)\n",
    "y_nslkdd_train = nslkdd_train['class']\n",
    "\n",
    "X_test = nslkdd_test.drop(['flag','service','protocol_type', 'class'], axis=1)\n",
    "y_test = nslkdd_test['class']\n",
    "y_test = y_test.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The reduced feature vector reduced_feature_vector was  generated using the XGBoost classifier in another notebook.\n",
    "\n",
    "#### In order to scale the data, there are two options:####\n",
    " 1. use the built-in  MinMaxScaler()\n",
    " 2. use custom built function scale_series() [Author: Dr Sydney Kasongo]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduced_feature_vector = ['src_bytes',\n",
    "                          'same_srv_rate',\n",
    "                          'dst_host_serror_rate',\n",
    "                          'dst_host_diff_srv_rate',\n",
    "                          'dst_host_srv_diff_host_rate',\n",
    "                          'count',\n",
    "                          'protocol_type_encoded',\n",
    "                          'diff_srv_rate',\n",
    "                          'dst_bytes',\n",
    "                          'dst_host_srv_count',\n",
    "                          'serror_rate',\n",
    "                          'wrong_fragment',\n",
    "                          'num_compromised',\n",
    "                          'service_encoded',\n",
    "                          'dst_host_same_src_port_rate',\n",
    "                          'hot',\n",
    "                          'num_file_creations',\n",
    "                          'dst_host_rerror_rate',\n",
    "                          'dst_host_count',\n",
    "                          'logged_in',\n",
    "                          'dst_host_same_srv_rate',\n",
    "                          'flag_encoded']\n",
    "\n",
    "X_nslkdd_train = X_nslkdd_train[reduced_feature_vector]\n",
    "X_test = X_test[reduced_feature_vector]\n",
    "\n",
    "#Generate the train and validation datasets.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_nslkdd_train, y_nslkdd_train, random_state=10)\n",
    "\n",
    "#Scaling \n",
    "scaler_train = MinMaxScaler()\n",
    "scaler_val = MinMaxScaler()\n",
    "scaler_test = MinMaxScaler()\n",
    "\n",
    "#Log Scaling\n",
    "def log_normalize(series):\n",
    "    return series.apply(lambda x:math.log(x+1.0))\n",
    "\n",
    "def scale_series(df, col_names):\n",
    "    processed_inputs = pd.DataFrame()\n",
    "    for col in col_names:\n",
    "        processed_inputs[col] =  log_normalize(df[col])\n",
    "    return df\n",
    "\n",
    "X_train = array(scale_series(X_train, reduced_feature_vector)).reshape(94479,1,22)\n",
    "X_val = array(scale_series(X_val, reduced_feature_vector)).reshape(31494,1,22)\n",
    "X_test = array(scale_series(X_test, reduced_feature_vector)).reshape(22544,1,22)\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the model and setup the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "dlstm_model = Sequential()\n",
    "dlstm_model.add(GRU(50,return_sequences=True, input_shape=(1,22)))\n",
    "dlstm_model.add(GRU(50,return_sequences=True))\n",
    "dlstm_model.add(GRU(50))\n",
    "dlstm_model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model\n",
    "dlstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',km.binary_precision(),km.binary_recall()]) #categorical_crossentropy mean_squared_error optimizer='adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start time of training\n",
    "start = time.time()\n",
    "# Training a model / Fit Model\n",
    "dlstm_neural_net_model= dlstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=500, verbose=0)\n",
    "#End time of training\n",
    "end = time.time()\n",
    "#Training Time\n",
    "delay = end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation Scores\n",
    "dlstm_validation_scores = dlstm_model.evaluate(X_val, y_val)\n",
    "#Test Scores\n",
    "dlstm_test_scores = dlstm_model.evaluate(X_test, y_test)\n",
    "\n",
    "#Compute f1_score - F Measure  \n",
    "precision = dlstm_validation_scores[2] * 100\n",
    "recall = dlstm_validation_scores[3] * 100\n",
    "\n",
    "f1score = (2 * precision * recall)/(precision + recall)\n",
    "\n",
    "print(\"LTSM Binary\")\n",
    "print(\"\\n DLSTM Test Accuracy: %.2f%%\" % (dlstm_test_scores[1]*100))\n",
    "print(\"Training Time \"+ str(delay))\n",
    "\n",
    "\n",
    "y_train_pred = dlstm_model.predict(X_train)\n",
    "y_test_pred = dlstm_model.predict(X_test)\n",
    "y_val_pred = dlstm_model.predict(X_val)\n",
    "\n",
    "\n",
    "cm_test = confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n",
    "\n",
    "\n",
    "cm_val = confusion_matrix(y_val.argmax(axis=1), y_val_pred.argmax(axis=1))\n",
    "cm_train = confusion_matrix(y_train.argmax(axis=1), y_train_pred.argmax(axis=1))\n",
    "\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm_test)\n",
    "plt.show()\n",
    "\n",
    "fig_1, ax_1 = plot_confusion_matrix(conf_mat=cm_val)\n",
    "plt.show()\n",
    "\n",
    "fig_2, ax_2 = plot_confusion_matrix(conf_mat=cm_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
